{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5632797-0363-42d8-a43c-2f982988d942",
   "metadata": {},
   "source": [
    "## Similarity Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25a562e-838d-4bed-ae52-ec12552b2406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blond\\anaconda3\\envs\\nlp_similarity\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7165c2-90ea-4381-9d3d-edf5c2e1a784",
   "metadata": {},
   "source": [
    "In this exercise, you've been provided the title and abstract of 500 recent machine learning research papers posted on arXiv.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7a796d-eaef-4e97-b787-b4bf5811c73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GoT-R1: Unleashing Reasoning Capability of MLL...</td>\n",
       "      <td>Visual generation models have made remarkable ...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17022v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delving into RL for Image Generation with CoT:...</td>\n",
       "      <td>Recent advancements underscore the significant...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17017v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interactive Post-Training for Vision-Language-...</td>\n",
       "      <td>We introduce RIPT-VLA, a simple and scalable r...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17016v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Are Concepts Erased From Diffusion Models?</td>\n",
       "      <td>Concept erasure, the ability to selectively pr...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17013v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Understanding Prompt Tuning and In-Context Lea...</td>\n",
       "      <td>Prompting is one of the main ways to adapt a p...</td>\n",
       "      <td>http://arxiv.org/abs/2505.17010v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  GoT-R1: Unleashing Reasoning Capability of MLL...   \n",
       "1  Delving into RL for Image Generation with CoT:...   \n",
       "2  Interactive Post-Training for Vision-Language-...   \n",
       "3    When Are Concepts Erased From Diffusion Models?   \n",
       "4  Understanding Prompt Tuning and In-Context Lea...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Visual generation models have made remarkable ...   \n",
       "1  Recent advancements underscore the significant...   \n",
       "2  We introduce RIPT-VLA, a simple and scalable r...   \n",
       "3  Concept erasure, the ability to selectively pr...   \n",
       "4  Prompting is one of the main ways to adapt a p...   \n",
       "\n",
       "                                 url  \n",
       "0  http://arxiv.org/abs/2505.17022v1  \n",
       "1  http://arxiv.org/abs/2505.17017v1  \n",
       "2  http://arxiv.org/abs/2505.17016v1  \n",
       "3  http://arxiv.org/abs/2505.17013v1  \n",
       "4  http://arxiv.org/abs/2505.17010v1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('../data/arxiv_papers.csv')\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e6836f-5f36-4b60-b21b-b9cbf7e5ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning\n",
      "\n",
      "Text: Visual generation models have made remarkable progress in creating realistic\n",
      "images from text prompts, yet struggle with complex prompts that specify\n",
      "multiple objects with precise spatial relationships and attributes. Effective\n",
      "handling of such prompts requires explicit reasoning about the semantic content\n",
      "and spatial layout. We present GoT-R1, a framework that applies reinforcement\n",
      "learning to enhance semantic-spatial reasoning in visual generation. Building\n",
      "upon the Generation Chain-of-Thought approach, GoT-R1 enables models to\n",
      "autonomously discover effective reasoning strategies beyond predefined\n",
      "templates through carefully designed reinforcement learning. To achieve this,\n",
      "we propose a dual-stage multi-dimensional reward framework that leverages MLLMs\n",
      "to evaluate both the reasoning process and final output, enabling effective\n",
      "supervision across the entire generation pipeline. The reward system assesses\n",
      "semantic alignment, spatial accuracy, and visual quality in a unified approach.\n",
      "Experimental results demonstrate significant improvements on T2I-CompBench\n",
      "benchmark, particularly in compositional tasks involving precise spatial\n",
      "relationships and attribute binding. GoT-R1 advances the state-of-the-art in\n",
      "image generation by successfully transferring sophisticated reasoning\n",
      "capabilities to the visual generation domain. To facilitate future research, we\n",
      "make our code and pretrained models publicly available at\n",
      "https://github.com/gogoduan/GoT-R1.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(f'Title: {articles.loc[i,\"title\"]}\\n')\n",
    "print(f'Text: {articles.loc[i,\"abstract\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee63919-08d4-400b-bb35-3e9f1b70088a",
   "metadata": {},
   "source": [
    "Let's try out a variety of ways of vectorizing and searching for semantically-similar papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32294485-637d-4b77-9a9d-2badcdfeb6dd",
   "metadata": {},
   "source": [
    "### Method 1: Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b788ee0-3070-4704-be92-8c442ff4c6d9",
   "metadata": {},
   "source": [
    "Fit a CountVectorizer to the abstracts of the articles with all of the defaults.  Then vectorize the dataset using the fit vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4cc272-474c-4b48-9e77-5a80cb13534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(articles['abstract'])\n",
    "cv_tf = cv.transform(articles['abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ee08b-50fa-468c-9655-47b110fa855b",
   "metadata": {},
   "source": [
    "**Question:** How many dimensions do the embeddings have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03208e89-aa86-4daa-bbd8-5952cfe2df2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7978)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_tf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eecfe606-937c-409e-823a-745fe380d407",
   "metadata": {},
   "source": [
    "Now, let's use the embeddings to look for similar articles to a search query.\n",
    "\n",
    "Apply the vectorizer you fit earlier to this query string to get an embedding. \n",
    "\n",
    "**Hint:** You can't pass a string to a vectorizer, but you can pass a list containing a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f38271b-eb14-4619-8f00-604bcf9bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"vector databases for retrieval augmented generation\"\n",
    "\n",
    "cv_query = cv.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dbea2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7978)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2c931-5dcd-49b7-aa8f-b5bdab82a541",
   "metadata": {},
   "source": [
    "Now, we need to find the similarity between our query embedding and each vectorized article.\n",
    "\n",
    "For this, you can use the [cosine similarity function from scikit-learn.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n",
    "\n",
    "Calculate the similarity between the query embedding and each article embedding and save the result to a variable named `similarity_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f558bfc-426f-4940-bebd-ce48398e48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = cosine_similarity(cv_query, cv_tf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b6443bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6eddc-b0a1-4fd0-8381-797313e2bcaa",
   "metadata": {},
   "source": [
    "Now, we need to find the most similar results. To help with this, we can use the [argsort function from numpy](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html), which will give the indices sorted by value. \n",
    "\n",
    "Use the argsort function to find the indices of the 5 most similar articles. Inspect their titles and abstracts. **Warning:** argsort sorts from smallest to largest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b3dffd0-5992-4879-a207-08dcc74afca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cbb80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indicies = indices[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a5a81ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([259, 289,  83, 486, 394])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72052256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIRB: Mathematical Information Retrieval Benchmark\n",
      "Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain.\n",
      "---\n",
      "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval\n",
      "Despite the dominance of convolutional and transformer-based architectures in\n",
      "image-to-image retrieval, these models are prone to biases arising from\n",
      "low-level visual features, such as color. Recognizing the lack of semantic\n",
      "understanding as a key limitation, we propose a novel scene graph-based\n",
      "retrieval framework that emphasizes semantic content over superficial image\n",
      "characteristics. Prior approaches to scene graph retrieval predominantly rely\n",
      "on supervised Graph Neural Networks (GNNs), which require ground truth graph\n",
      "pairs driven from image captions. However, the inconsistency of caption-based\n",
      "supervision stemming from variable text encodings undermine retrieval\n",
      "reliability. To address these, we present SCENIR, a Graph Autoencoder-based\n",
      "unsupervised retrieval framework, which eliminates the dependence on labeled\n",
      "training data. Our model demonstrates superior performance across metrics and\n",
      "runtime efficiency, outperforming existing vision-based, multimodal, and\n",
      "supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\n",
      "a deterministic and robust ground truth measure for scene graph similarity,\n",
      "replacing the inconsistent caption-based alternatives for the first time in\n",
      "image-to-image retrieval evaluation. Finally, we validate the generalizability\n",
      "of our method by applying it to unannotated datasets via automated scene graph\n",
      "generation, while substantially contributing in advancing state-of-the-art in\n",
      "counterfactual image retrieval.\n",
      "---\n",
      "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning\n",
      "Tabular data, ubiquitous and rich in informational value, is an increasing\n",
      "focus for deep representation learning, yet progress is hindered by studies\n",
      "centered on single tables or isolated databases, which limits model\n",
      "capabilities due to data scale. While collaborative learning approaches such as\n",
      "federated learning, transfer learning, split learning, and tabular foundation\n",
      "models aim to learn from multiple correlated databases, they are challenged by\n",
      "a scarcity of real-world interconnected tabular resources. Current data lakes\n",
      "and corpora largely consist of isolated databases lacking defined\n",
      "inter-database correlations. To overcome this, we introduce WikiDBGraph, a\n",
      "large-scale graph of 100,000 real-world tabular databases from WikiData,\n",
      "interconnected by 17 million edges and characterized by 13 node and 12 edge\n",
      "properties derived from its database schema and data distribution.\n",
      "WikiDBGraph's weighted edges identify both instance- and feature-overlapped\n",
      "databases. Experiments on these newly identified databases confirm that\n",
      "collaborative learning yields superior performance, thereby offering\n",
      "considerable promise for structured foundation model training while also\n",
      "exposing key challenges and future directions for learning from interconnected\n",
      "tabular data.\n",
      "---\n",
      "Explaining Neural Networks with Reasons\n",
      "We propose a new interpretability method for neural networks, which is based\n",
      "on a novel mathematico-philosophical theory of reasons. Our method computes a\n",
      "vector for each neuron, called its reasons vector. We then can compute how\n",
      "strongly this reasons vector speaks for various propositions, e.g., the\n",
      "proposition that the input image depicts digit 2 or that the input prompt has a\n",
      "negative sentiment. This yields an interpretation of neurons, and groups\n",
      "thereof, that combines a logical and a Bayesian perspective, and accounts for\n",
      "polysemanticity (i.e., that a single neuron can figure in multiple concepts).\n",
      "We show, both theoretically and empirically, that this method is: (1) grounded\n",
      "in a philosophically established notion of explanation, (2) uniform, i.e.,\n",
      "applies to the common neural network architectures and modalities, (3)\n",
      "scalable, since computing reason vectors only involves forward-passes in the\n",
      "neural network, (4) faithful, i.e., intervening on a neuron based on its reason\n",
      "vector leads to expected changes in model output, (5) correct in that the\n",
      "model's reasons structure matches that of the data source, (6) trainable, i.e.,\n",
      "neural networks can be trained to improve their reason strengths, (7) useful,\n",
      "i.e., it delivers on the needs for interpretability by increasing, e.g.,\n",
      "robustness and fairness.\n",
      "---\n",
      "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning\n",
      "Autonomous robots must reason about the physical consequences of their\n",
      "actions to operate effectively in unstructured, real-world environments. We\n",
      "present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D\n",
      "Gaussian Splatting for accurate scene reconstruction, visual foundation models\n",
      "for semantic segmentation, vision-language models for material property\n",
      "inference, and physics simulation for reliable prediction of action outcomes.\n",
      "By integrating these components, SMS enables generalizable physical reasoning\n",
      "and object-centric planning without the need to re-learn foundational physical\n",
      "dynamics. We empirically validate SMS in a billiards-inspired manipulation task\n",
      "and a challenging quadrotor landing scenario, demonstrating robust performance\n",
      "on both simulated domain transfer and real-world experiments. Our results\n",
      "highlight the potential of bridging differentiable rendering for scene\n",
      "reconstruction, foundation models for semantic understanding, and physics-based\n",
      "simulation to achieve physically grounded robot planning across diverse\n",
      "settings.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in top_indicies: \n",
    "    print(articles.loc[i, 'title'])\n",
    "    print(articles.loc[i, 'abstract']) \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94c418-8156-4e76-b41b-26276400a92a",
   "metadata": {},
   "source": [
    "Try using a tfidf vectorizer. How do the results compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "020ca2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(articles['abstract'])\n",
    "tfidf_tf = tfidf.transform(articles['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63def98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7978)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c4b5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_query = tfidf.transform([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4839b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_t = cosine_similarity(tfidf_query, tfidf_tf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ca6098c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab50f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_t = np.argsort(similarity_scores_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2bf71304",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indicies_t = indices_t[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1774bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([259,  83, 289, 233, 486])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indicies_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d195f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIRB: Mathematical Information Retrieval Benchmark\n",
      "Mathematical Information Retrieval (MIR) is the task of retrieving\n",
      "information from mathematical documents and plays a key role in various\n",
      "applications, including theorem search in mathematical libraries, answer\n",
      "retrieval on math forums, and premise selection in automated theorem proving.\n",
      "However, a unified benchmark for evaluating these diverse retrieval tasks has\n",
      "been lacking. In this paper, we introduce MIRB (Mathematical Information\n",
      "Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\n",
      "includes four tasks: semantic statement retrieval, question-answer retrieval,\n",
      "premise retrieval, and formula retrieval, spanning a total of 12 datasets. We\n",
      "evaluate 13 retrieval models on this benchmark and analyze the challenges\n",
      "inherent to MIR. We hope that MIRB provides a comprehensive framework for\n",
      "evaluating MIR systems and helps advance the development of more effective\n",
      "retrieval models tailored to the mathematical domain.\n",
      "---\n",
      "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning\n",
      "Tabular data, ubiquitous and rich in informational value, is an increasing\n",
      "focus for deep representation learning, yet progress is hindered by studies\n",
      "centered on single tables or isolated databases, which limits model\n",
      "capabilities due to data scale. While collaborative learning approaches such as\n",
      "federated learning, transfer learning, split learning, and tabular foundation\n",
      "models aim to learn from multiple correlated databases, they are challenged by\n",
      "a scarcity of real-world interconnected tabular resources. Current data lakes\n",
      "and corpora largely consist of isolated databases lacking defined\n",
      "inter-database correlations. To overcome this, we introduce WikiDBGraph, a\n",
      "large-scale graph of 100,000 real-world tabular databases from WikiData,\n",
      "interconnected by 17 million edges and characterized by 13 node and 12 edge\n",
      "properties derived from its database schema and data distribution.\n",
      "WikiDBGraph's weighted edges identify both instance- and feature-overlapped\n",
      "databases. Experiments on these newly identified databases confirm that\n",
      "collaborative learning yields superior performance, thereby offering\n",
      "considerable promise for structured foundation model training while also\n",
      "exposing key challenges and future directions for learning from interconnected\n",
      "tabular data.\n",
      "---\n",
      "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval\n",
      "Despite the dominance of convolutional and transformer-based architectures in\n",
      "image-to-image retrieval, these models are prone to biases arising from\n",
      "low-level visual features, such as color. Recognizing the lack of semantic\n",
      "understanding as a key limitation, we propose a novel scene graph-based\n",
      "retrieval framework that emphasizes semantic content over superficial image\n",
      "characteristics. Prior approaches to scene graph retrieval predominantly rely\n",
      "on supervised Graph Neural Networks (GNNs), which require ground truth graph\n",
      "pairs driven from image captions. However, the inconsistency of caption-based\n",
      "supervision stemming from variable text encodings undermine retrieval\n",
      "reliability. To address these, we present SCENIR, a Graph Autoencoder-based\n",
      "unsupervised retrieval framework, which eliminates the dependence on labeled\n",
      "training data. Our model demonstrates superior performance across metrics and\n",
      "runtime efficiency, outperforming existing vision-based, multimodal, and\n",
      "supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\n",
      "a deterministic and robust ground truth measure for scene graph similarity,\n",
      "replacing the inconsistent caption-based alternatives for the first time in\n",
      "image-to-image retrieval evaluation. Finally, we validate the generalizability\n",
      "of our method by applying it to unannotated datasets via automated scene graph\n",
      "generation, while substantially contributing in advancing state-of-the-art in\n",
      "counterfactual image retrieval.\n",
      "---\n",
      "HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases\n",
      "Large Language Models (LLMs) have demonstrated their potential in hardware\n",
      "design tasks, such as Hardware Description Language (HDL) generation and\n",
      "debugging. Yet, their performance in real-world, repository-level HDL projects\n",
      "with thousands or even tens of thousands of code lines is hindered. To this\n",
      "end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval\n",
      "Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph\n",
      "representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow\n",
      "Graphs (DFGs) to capture both code graph view and hardware graph view.\n",
      "HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the\n",
      "limited recall issues inherent in similarity-based semantic retrieval by\n",
      "incorporating structural information, but also enhances its extensibility to\n",
      "various real-world tasks by a task-specific retrieval finetuning. Additionally,\n",
      "to address the lack of comprehensive HDL search benchmarks, we introduce\n",
      "HDLSearch, a multi-granularity evaluation dataset derived from real-world\n",
      "repository-level projects. Experimental results demonstrate that HDLxGraph\n",
      "significantly improves average search accuracy, debugging efficiency and\n",
      "completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based\n",
      "RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are\n",
      "available at https://github.com/Nick-Zheng-Q/HDLxGraph.\n",
      "---\n",
      "Explaining Neural Networks with Reasons\n",
      "We propose a new interpretability method for neural networks, which is based\n",
      "on a novel mathematico-philosophical theory of reasons. Our method computes a\n",
      "vector for each neuron, called its reasons vector. We then can compute how\n",
      "strongly this reasons vector speaks for various propositions, e.g., the\n",
      "proposition that the input image depicts digit 2 or that the input prompt has a\n",
      "negative sentiment. This yields an interpretation of neurons, and groups\n",
      "thereof, that combines a logical and a Bayesian perspective, and accounts for\n",
      "polysemanticity (i.e., that a single neuron can figure in multiple concepts).\n",
      "We show, both theoretically and empirically, that this method is: (1) grounded\n",
      "in a philosophically established notion of explanation, (2) uniform, i.e.,\n",
      "applies to the common neural network architectures and modalities, (3)\n",
      "scalable, since computing reason vectors only involves forward-passes in the\n",
      "neural network, (4) faithful, i.e., intervening on a neuron based on its reason\n",
      "vector leads to expected changes in model output, (5) correct in that the\n",
      "model's reasons structure matches that of the data source, (6) trainable, i.e.,\n",
      "neural networks can be trained to improve their reason strengths, (7) useful,\n",
      "i.e., it delivers on the needs for interpretability by increasing, e.g.,\n",
      "robustness and fairness.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in top_indicies_t: \n",
    "    print(articles.loc[i, 'title'])\n",
    "    print(articles.loc[i, 'abstract']) \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed45785-5b64-4234-935a-2bb6e3636d5c",
   "metadata": {},
   "source": [
    "### Method 2: Using a Pretrained Embedding Model\n",
    "\n",
    "Now, let's compare how we do using the [all-MiniLM-L6-v2 embedding model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "This will create a 384-dimensional dense embedding of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c08b104a-2599-4f02-8bd0-147205f3ffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blond\\anaconda3\\envs\\nlp_similarity\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\blond\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "369b82b2-a0a3-44be-b89b-097d72a53521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.76569641e-02  6.34958968e-02  4.87131327e-02  7.93049932e-02\n",
      "   3.74479853e-02  2.65277526e-03  3.93749401e-02 -7.09840422e-03\n",
      "   5.93614280e-02  3.15369591e-02  6.00980893e-02 -5.29051833e-02\n",
      "   4.06068265e-02 -2.59308796e-02  2.98428163e-02  1.12684851e-03\n",
      "   7.35149309e-02 -5.03819138e-02 -1.22386634e-01  2.37028524e-02\n",
      "   2.97265034e-02  4.24768254e-02  2.56337989e-02  1.99519284e-03\n",
      "  -5.69191016e-02 -2.71598455e-02 -3.29035446e-02  6.60248697e-02\n",
      "   1.19007058e-01 -4.58790585e-02 -7.26214871e-02 -3.25839929e-02\n",
      "   5.23413904e-02  4.50552516e-02  8.25305562e-03  3.67023498e-02\n",
      "  -1.39415562e-02  6.53919354e-02 -2.64272839e-02  2.06388984e-04\n",
      "  -1.36643872e-02 -3.62809822e-02 -1.95043311e-02 -2.89738625e-02\n",
      "   3.94270457e-02 -8.84090886e-02  2.62431079e-03  1.36714112e-02\n",
      "   4.83063310e-02 -3.11565101e-02 -1.17329173e-01 -5.11690043e-02\n",
      "  -8.85288417e-02 -2.18962971e-02  1.42986458e-02  4.44168039e-02\n",
      "  -1.34814847e-02  7.43392557e-02  2.66382769e-02 -1.98762212e-02\n",
      "   1.79190934e-02 -1.06052384e-02 -9.04263631e-02  2.13269852e-02\n",
      "   1.41204834e-01 -6.47181366e-03 -1.40386552e-03 -1.53609589e-02\n",
      "  -8.73571783e-02  7.22174346e-02  2.01402809e-02  4.25587520e-02\n",
      "  -3.49013731e-02  3.19601386e-04 -8.02970976e-02 -3.27473544e-02\n",
      "   2.85268463e-02 -5.13658151e-02  1.09389216e-01  8.19327906e-02\n",
      "  -9.84040126e-02 -9.34094340e-02 -1.51292216e-02  4.51248363e-02\n",
      "   4.94171418e-02 -2.51867957e-02  1.57077648e-02 -1.29290715e-01\n",
      "   5.31889405e-03  4.02346067e-03 -2.34572440e-02 -6.72982931e-02\n",
      "   2.92281397e-02 -2.60845665e-02  1.30625311e-02 -3.11662611e-02\n",
      "  -4.82714176e-02 -5.58859371e-02 -3.87505591e-02  1.20010763e-01\n",
      "  -1.03924163e-02  4.89704944e-02  5.53536825e-02  4.49358858e-02\n",
      "  -4.00964636e-03 -1.02959707e-01 -2.92968582e-02 -5.83401807e-02\n",
      "   2.70472318e-02 -2.20169909e-02 -7.22241253e-02 -4.13869768e-02\n",
      "  -1.93298273e-02  2.73335143e-03  2.76874373e-04 -9.67588127e-02\n",
      "  -1.00574732e-01 -1.41922589e-02 -8.07891712e-02  4.53925170e-02\n",
      "   2.45041568e-02  5.97613715e-02 -7.38185421e-02  1.19843595e-02\n",
      "  -6.63403198e-02 -7.69045204e-02  3.85157429e-02 -5.59362036e-33\n",
      "   2.80013122e-02 -5.60785010e-02 -4.86602075e-02  2.15569548e-02\n",
      "   6.01981059e-02 -4.81403209e-02 -3.50246504e-02  1.93313807e-02\n",
      "  -1.75151695e-02 -3.89209464e-02 -3.81058268e-03 -1.70287732e-02\n",
      "   2.82099340e-02  1.28290895e-02  4.71601039e-02  6.21029921e-02\n",
      "  -6.43588975e-02  1.29285634e-01 -1.31231602e-02  5.23069650e-02\n",
      "  -3.73681076e-02  2.89093684e-02 -1.68980993e-02 -2.37330012e-02\n",
      "  -3.33491825e-02 -5.16763031e-02  1.55356517e-02  2.08802521e-02\n",
      "  -1.25371451e-02  4.59579118e-02  3.72720808e-02  2.80566942e-02\n",
      "  -5.90005890e-02 -1.16987703e-02  4.92182523e-02  4.70328741e-02\n",
      "   7.35487714e-02 -3.70529890e-02  3.98465712e-03  1.06412023e-02\n",
      "  -1.61598990e-04 -5.27166016e-02  2.75927596e-02 -3.92921716e-02\n",
      "   8.44718218e-02  4.86859903e-02 -4.85874573e-03  1.79948732e-02\n",
      "  -4.28570174e-02  1.23375272e-02  6.39956584e-03  4.04822975e-02\n",
      "   1.48887346e-02 -1.53941428e-02  7.62946978e-02  2.37043630e-02\n",
      "   4.45237830e-02  5.08195162e-02 -2.31254776e-03 -1.88737363e-02\n",
      "  -1.23336026e-02  4.66002040e-02 -5.63438162e-02  6.29927069e-02\n",
      "  -3.15534994e-02  3.24912779e-02  2.34672874e-02 -6.55438304e-02\n",
      "   2.01709289e-02  2.57082321e-02 -1.23868398e-02 -8.36492423e-03\n",
      "  -6.64378181e-02  9.43073854e-02 -3.57093476e-02 -3.42483148e-02\n",
      "  -6.66353060e-03 -8.01524054e-03 -3.09711304e-02  4.33011651e-02\n",
      "  -8.21396057e-03 -1.50795028e-01  3.07692308e-02  4.00718898e-02\n",
      "  -3.79293114e-02  1.93219341e-03  4.00530249e-02 -8.77074674e-02\n",
      "  -3.68491448e-02  8.57955031e-03 -3.19251940e-02 -1.25258481e-02\n",
      "   7.35538825e-02  1.34735729e-03  2.05918513e-02  2.71097981e-33\n",
      "  -5.18576838e-02  5.78360707e-02 -9.18985680e-02  3.94421928e-02\n",
      "   1.05576508e-01 -1.96912512e-02  6.18402883e-02 -7.63465017e-02\n",
      "   2.40880493e-02  9.40049216e-02 -1.16535515e-01  3.71198095e-02\n",
      "   5.22425100e-02 -3.95852700e-03  5.72214611e-02  5.32853883e-03\n",
      "   1.24016821e-01  1.39022395e-02 -1.10249976e-02  3.56053039e-02\n",
      "  -3.30755413e-02  8.16573352e-02 -1.52003942e-02  6.05585575e-02\n",
      "  -6.01397641e-02  3.26102972e-02 -3.48296501e-02 -1.69882197e-02\n",
      "  -9.74908099e-02 -2.71484088e-02  1.74713321e-03 -7.68982247e-02\n",
      "  -4.31858264e-02 -1.89985204e-02 -2.91661434e-02  5.77487983e-02\n",
      "   2.41821557e-02 -1.16901929e-02 -6.21435195e-02  2.84351520e-02\n",
      "  -2.37513959e-04 -2.51783449e-02  4.39627748e-03  8.12840462e-02\n",
      "   3.64184529e-02 -6.04005978e-02 -3.65517512e-02 -7.93748423e-02\n",
      "  -5.08527271e-03  6.69699460e-02 -1.17784418e-01  3.23744044e-02\n",
      "  -4.71252240e-02 -1.34459911e-02 -9.48445350e-02  8.24941974e-03\n",
      "  -1.06748380e-02 -6.81882128e-02  1.11817243e-03  2.48019584e-02\n",
      "  -6.35889396e-02  2.84492522e-02 -2.61303522e-02  8.58111009e-02\n",
      "   1.14682250e-01 -5.35345115e-02 -5.63588217e-02  4.26009074e-02\n",
      "   1.09453639e-02  2.09579263e-02  1.00131184e-01  3.26051898e-02\n",
      "  -1.84208751e-01 -3.93208191e-02 -6.91454336e-02 -6.38104901e-02\n",
      "  -6.56385794e-02 -6.41248701e-03 -4.79612425e-02 -7.68133029e-02\n",
      "   2.95383129e-02 -2.29948554e-02  4.17036600e-02 -2.50047706e-02\n",
      "  -4.54509770e-03 -4.17136699e-02 -1.32289147e-02 -6.38357550e-02\n",
      "  -2.46471050e-03 -1.37337893e-02  1.68977268e-02 -6.30397946e-02\n",
      "   8.98881108e-02  4.18170765e-02 -1.85687672e-02 -1.80442168e-08\n",
      "  -1.67998318e-02 -3.21578160e-02  6.30384460e-02 -4.13092114e-02\n",
      "   4.44819480e-02  2.02471367e-03  6.29593283e-02 -5.17368037e-03\n",
      "  -1.00444136e-02 -3.05640604e-02  3.52672003e-02  5.58580831e-02\n",
      "  -4.67125177e-02  3.45103405e-02  3.29577848e-02  4.30114269e-02\n",
      "   2.94361599e-02 -3.03164031e-02 -1.71107464e-02  7.37486109e-02\n",
      "  -5.47910370e-02  2.77515259e-02  6.20161276e-03  1.58800613e-02\n",
      "   3.42978351e-02 -5.15747024e-03  2.35079732e-02  7.53135681e-02\n",
      "   1.92843303e-02  3.36196870e-02  5.09104021e-02  1.52497068e-01\n",
      "   1.64207947e-02  2.70528384e-02  3.75162587e-02  2.18553208e-02\n",
      "   5.66333942e-02 -3.95747796e-02  7.12312981e-02 -5.41376993e-02\n",
      "   1.03779952e-03  2.11853664e-02 -3.56307998e-02  1.09017029e-01\n",
      "   2.76528462e-03  3.13997157e-02  1.38423126e-03 -3.45737711e-02\n",
      "  -4.59277742e-02  2.88083237e-02  7.16902083e-03  4.84685227e-02\n",
      "   2.61017848e-02 -9.44073591e-03  2.82169133e-02  3.48723754e-02\n",
      "   3.69099006e-02 -8.58957227e-03 -3.53205316e-02 -2.47856788e-02\n",
      "  -1.91921182e-02  3.80707644e-02  5.99653497e-02 -4.22286615e-02]\n",
      " [ 8.64385888e-02  1.02762640e-01  5.39451977e-03  2.04439671e-03\n",
      "  -9.96339507e-03  2.53854636e-02  4.92875800e-02 -3.06265727e-02\n",
      "   6.87254295e-02  1.01366173e-02  7.75397941e-02 -9.00806934e-02\n",
      "   6.10618480e-03 -5.69898263e-02  1.41715314e-02  2.80491095e-02\n",
      "  -8.68464410e-02  7.64399245e-02 -1.03491306e-01 -6.77437782e-02\n",
      "   6.99947029e-02  8.44251066e-02 -7.24915322e-03  1.04770670e-02\n",
      "   1.34020476e-02  6.77577034e-02 -9.42085832e-02 -3.71690169e-02\n",
      "   5.22617213e-02 -3.10853403e-02 -9.63406563e-02  1.57717075e-02\n",
      "   2.57867016e-02  7.85244778e-02  7.89949149e-02  1.91516783e-02\n",
      "   1.64356641e-02  3.10077984e-03  3.81311290e-02  2.37090290e-02\n",
      "   1.05389757e-02 -4.40644696e-02  4.41738293e-02 -2.58727968e-02\n",
      "   6.15379177e-02 -4.05427590e-02 -8.64140615e-02  3.19722816e-02\n",
      "  -8.90662661e-04 -2.44436897e-02 -9.19721723e-02  2.33939439e-02\n",
      "  -8.30293298e-02  4.41510454e-02 -2.49693263e-02  6.23020753e-02\n",
      "  -1.30350108e-03  7.51394853e-02  2.46385094e-02 -6.47244751e-02\n",
      "  -1.17727771e-01  3.83392237e-02 -9.11767334e-02  6.35446310e-02\n",
      "   7.62739405e-02 -8.80241543e-02  9.54555906e-03 -4.69717234e-02\n",
      "  -8.41740593e-02  3.88823599e-02 -1.14393584e-01  6.28860295e-03\n",
      "  -3.49361673e-02  2.39750817e-02 -3.31317261e-02 -1.57244485e-02\n",
      "  -3.78955677e-02 -8.81248061e-03  7.06119239e-02  3.28066237e-02\n",
      "   2.03674380e-03 -1.12278916e-01  6.79721823e-03  1.22765247e-02\n",
      "   3.35303284e-02 -1.36200441e-02 -2.25490090e-02 -2.25228835e-02\n",
      "  -2.03194339e-02  5.04297167e-02 -7.48652965e-02 -8.22822005e-02\n",
      "   7.65962377e-02  4.93392237e-02 -3.75553593e-02  1.44634778e-02\n",
      "  -5.72457314e-02 -1.79954655e-02  1.09697953e-01  1.19462833e-01\n",
      "   8.09196383e-04  6.17057793e-02  3.26322056e-02 -1.30780086e-01\n",
      "  -1.48636654e-01 -6.16232865e-02  4.33886088e-02  2.67128963e-02\n",
      "   1.39786061e-02 -3.94002274e-02 -2.52711382e-02  3.87746957e-03\n",
      "   3.58664729e-02 -6.15420677e-02  3.76660787e-02  2.67565008e-02\n",
      "  -3.82658951e-02 -3.54793333e-02 -2.39227358e-02  8.67977291e-02\n",
      "  -1.84063204e-02  7.71039277e-02  1.39855337e-03  7.00382814e-02\n",
      "  -4.77877818e-02 -7.89820105e-02  5.10814190e-02 -2.99868315e-33\n",
      "  -3.91646288e-02 -2.56213173e-03  1.65210329e-02  9.48941614e-03\n",
      "  -5.66219464e-02  6.57783076e-02 -4.77002487e-02  1.11661693e-02\n",
      "  -5.73558360e-02 -9.16259829e-03 -2.17521247e-02 -5.59532195e-02\n",
      "  -1.11422557e-02  9.32793766e-02  1.66765060e-02 -1.36723872e-02\n",
      "   4.34387848e-02  1.87247247e-03  7.29949167e-03  5.16332462e-02\n",
      "   4.80608046e-02  1.35341480e-01 -1.71739329e-02 -1.29698236e-02\n",
      "  -7.50109777e-02  2.61107739e-02  2.69801747e-02  7.83018942e-04\n",
      "  -4.87269610e-02  1.17842807e-02 -4.59579900e-02 -4.83213700e-02\n",
      "  -1.95671432e-02  1.93889067e-02  1.98807362e-02  1.67432353e-02\n",
      "   9.87801105e-02 -2.74087749e-02  2.34808717e-02  3.70232924e-03\n",
      "  -6.14514649e-02 -1.21232262e-03 -9.50472057e-03  9.25156847e-03\n",
      "   2.38443762e-02  8.61232430e-02  2.26790085e-02  5.45093499e-04\n",
      "   3.47130150e-02  6.25458919e-03 -6.92777615e-03  3.92400473e-02\n",
      "   1.15674771e-02  3.26279514e-02  6.22155145e-02  2.76114475e-02\n",
      "   1.86884329e-02  3.55805419e-02  4.11796197e-02  1.54782021e-02\n",
      "   4.22691219e-02  3.82248461e-02  1.00313397e-02 -2.83245780e-02\n",
      "   4.47052568e-02 -4.10459079e-02 -4.50551696e-03 -5.44734709e-02\n",
      "   2.62321103e-02  1.79862175e-02 -1.23118781e-01 -4.66951914e-02\n",
      "  -1.35913370e-02  6.46710470e-02  3.57345329e-03 -1.22234020e-02\n",
      "  -1.79382134e-02 -2.55502388e-02  2.37223972e-02  4.08666627e-03\n",
      "  -6.51475638e-02  4.43651453e-02  4.68595847e-02 -3.25174965e-02\n",
      "   4.02274728e-03 -3.97604844e-03  1.11939637e-02 -9.95597914e-02\n",
      "   3.33168395e-02  8.01060647e-02  9.42692384e-02 -6.38294443e-02\n",
      "   3.23151797e-02 -5.13553321e-02 -7.49871228e-03  5.30047806e-34\n",
      "  -4.13194299e-02  9.49646756e-02 -1.06401429e-01  4.96590622e-02\n",
      "  -3.41913737e-02 -3.16746421e-02 -1.71556044e-02  1.70100725e-03\n",
      "   5.79757988e-02 -1.21776760e-03 -1.68536119e-02 -5.16912788e-02\n",
      "   5.52998781e-02 -3.42646837e-02  3.08179446e-02 -3.10481209e-02\n",
      "   9.27532390e-02  3.72663774e-02 -2.37397868e-02  4.45893481e-02\n",
      "   1.46153504e-02  1.16239369e-01 -5.00112399e-02  3.88716087e-02\n",
      "   4.24750708e-03  2.56976262e-02  3.27243991e-02  4.29907888e-02\n",
      "  -1.36144636e-02  2.56122034e-02  1.06262080e-02 -8.46864209e-02\n",
      "  -9.52982455e-02  1.08399905e-01 -7.51600489e-02 -1.37773827e-02\n",
      "   6.37337714e-02 -4.49674157e-03 -3.25321406e-02  6.23613335e-02\n",
      "   3.48052830e-02 -3.54922228e-02 -2.00222172e-02  3.66608202e-02\n",
      "  -2.48837173e-02  1.01818601e-02 -7.01232702e-02 -4.31950726e-02\n",
      "   2.95332912e-02 -2.94983707e-04 -3.45386267e-02  1.46676172e-02\n",
      "  -9.83969867e-02 -4.70488258e-02 -8.85494705e-03 -8.89914334e-02\n",
      "   3.50995697e-02 -1.29602030e-01 -4.98865843e-02 -6.12047762e-02\n",
      "  -5.97797260e-02  9.46319848e-03  4.91218008e-02 -7.75026083e-02\n",
      "   8.09727237e-02 -4.79257554e-02  2.34376267e-03  7.57031366e-02\n",
      "  -2.40175836e-02 -1.52546121e-02  4.86738347e-02 -3.85968350e-02\n",
      "  -7.04831481e-02 -1.20347869e-02 -3.88790406e-02 -7.76017159e-02\n",
      "  -1.07244011e-02  1.04188174e-02 -2.13753656e-02 -9.17385966e-02\n",
      "  -1.11344811e-02 -2.96065994e-02  2.46457830e-02  4.65711672e-03\n",
      "  -1.63449887e-02 -3.95219624e-02  7.73373470e-02 -2.84732934e-02\n",
      "  -3.69941816e-03  8.27665329e-02 -1.10409008e-02  3.13983671e-02\n",
      "   5.35094254e-02  5.75145632e-02 -3.17621790e-02 -1.52911284e-08\n",
      "  -7.99661651e-02 -4.76797558e-02 -8.59788582e-02  5.69616519e-02\n",
      "  -4.08866405e-02  2.23832596e-02 -4.64442698e-03 -3.80131193e-02\n",
      "  -3.10671292e-02 -1.07278330e-02  1.97698530e-02  7.76996883e-03\n",
      "  -6.09474955e-03 -3.86376418e-02  2.80272141e-02  6.78138062e-02\n",
      "  -2.35350728e-02  3.21747735e-02  8.02538637e-03 -2.39106864e-02\n",
      "  -1.21997623e-03  3.14598978e-02 -5.24923988e-02 -8.06806982e-03\n",
      "   3.14769777e-03  5.11496775e-02 -4.44104560e-02  6.36013448e-02\n",
      "   3.85084301e-02  3.30432616e-02 -4.18725703e-03  4.95592691e-02\n",
      "  -5.69604710e-02 -6.49713818e-03 -2.49793157e-02 -1.60866808e-02\n",
      "   6.62289634e-02 -2.06310693e-02  1.08045779e-01  1.68546867e-02\n",
      "   1.43813072e-02 -1.32126911e-02 -1.29387394e-01  6.95216358e-02\n",
      "  -5.55773266e-02 -6.75413981e-02 -5.45818591e-03 -6.13594521e-03\n",
      "   3.90841439e-02 -6.28779829e-02  3.74063589e-02 -1.16570937e-02\n",
      "   1.29150357e-02 -5.52495793e-02  5.16075604e-02 -4.30839648e-03\n",
      "   5.80247194e-02  1.86945125e-02  2.27810610e-02  3.21666263e-02\n",
      "   5.37978783e-02  7.02849254e-02  7.49312118e-02 -8.41775015e-02]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "embeddings = embedder.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf339d4b-23e6-4034-84bf-7198135f758a",
   "metadata": {},
   "source": [
    "Use this new embedder to vectorize the abstracts and then find the most similar to the query. How do the results compare to the other methods?\n",
    "\n",
    "**Warning:** Creating embeddings for all of the articles may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ceb666a0-11b3-42fb-b4d0-2e9b5d83dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_emb = embedder.encode(articles['abstract'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "515a5588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 384)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10880f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_query = embedder.encode([query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4faf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_s = cosine_similarity(sent_query, sent_emb)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28ed6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_s = np.argsort(similarity_scores_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea8ea3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indicies_s = indices[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9e35bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([259,  83, 289, 233, 486])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_indicies_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d62bc31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIRB: Mathematical Information Retrieval Benchmark\n",
      "---\n",
      "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval\n",
      "---\n",
      "WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning\n",
      "---\n",
      "Explaining Neural Networks with Reasons\n",
      "---\n",
      "Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in top_indicies: \n",
    "    print(articles.loc[i, 'title'])\n",
    "    # print(articles.loc[i, 'abstract']) \n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8dec64-2aa7-40af-adfd-230c19671593",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "\n",
    "The [Faiss library](https://faiss.ai/index.html) is a library for efficient similarity search and clustering of dense vectors. It can be used to automate the process of finding the most similar abstracts.\n",
    "\n",
    "If we want to use cosine similarity, we need to use the Inner Product. We also need to normalize our vectors so that they all have length 1.\n",
    "\n",
    "Use the [normalize function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) to normalize both the abstract vectors and the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5225fb-e429-4337-92f4-9cdb2468ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bb61e-471a-4660-99d7-e3e513d34098",
   "metadata": {},
   "source": [
    "Now, create an [IndexFlatIP object](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#summary-of-methods) that has dimensions equal to the dimensionality of your vectors. Then add your normalized abstract vectors.\n",
    "\n",
    "Hint: You can mimic the example [here](https://github.com/facebookresearch/faiss/wiki/Getting-started#building-an-index-and-adding-the-vectors-to-it), but substitute in the IndexFlatIP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7cc7f-8889-4b0c-8bd5-54c760ab4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e3f95-6f37-4918-800a-094314b9554a",
   "metadata": {},
   "source": [
    "Finally, use the [search function](https://github.com/facebookresearch/faiss/wiki/Getting-started#searching) on your index object to find the 5 most similar articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884c016-444b-4714-85fc-07a8cdaf2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_similarity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
